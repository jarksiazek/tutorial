### S3 ###
* infinitely scaling storage
* many website use S3 as a backbone
* many services uses S3 as an integration as well

### Overview ###
* buckets
    * allows storing objects (files) in "buckets" (directories)
    * bucket must have a global unique name
    * buckets are defined at the region level
    * naming convention
        * no uppercase
        * no underscore
        * 3-36 characters
        * not an IP
        * must start with lowerCase letter or number
* objects
    * objects (files) have a key and body
    * key is the FULL path
        * s3://my-bucket/my_file.txt
        * s3://my-bucket/my_folder/my_file.txt
    * the key is composed of "prefix + object name"
    * body 
        * max size is 5TB
        * upload more than 5GB, must use "multi-part upload"
    * metadata
        * list of text key/value pairs - system or user metadata
    * tags (unicode key/ value pair - up to 10) - useful for security/lifecycle
    * version ID (if versioning is enabled)

### S3 Versioning ###
https://www.udemy.com/course/aws-certified-developer-associate-dva-c01/learn/lecture/11851526#content
* file can be versioning
* default version is "null" - versioning enabled
* it is enabled at the bucket level
* same key with a new version of the file - copy
* it is the best practice to version your buckets
    * protect against unintended deletes
    * easy roll back to previous version
     
### S3 Encrypting ###
https://www.udemy.com/course/aws-certified-developer-associate-dva-c01/learn/lecture/11851534#content       
* 4 methods of encrypting objects in S3
    * SSE-S3 
        ![](images/aim1.jpg) 
        * encrypts S3 objects using keys handled and managed by AWS
        * object is encrypted server side
        * AES-256 encryption type
        * must set header "x-amx-server-side-encryption":"AES256"
    * SSE-KMS 
        ![](images/aim2.jpg) 
        * leverage AWS Key Management Service to manage encryption keys
        * KMS advantages: user control + audit trail
        * object is encrypted server side
        * must set header "x-amx-server-side-encryption":"aws:kms"
    * SSE-C 
        ![](images/aim3.jpg) 
        * server-side encryption using data keys fully managed by customer outside of AWS
        * S3 does not store the encryption key you provide
        * HTTPS must be used
        * Encryption key must provided in HTTP headers for every HTTP request made
    * Client Side Encryption 
        ![](images/aim4.jpg) 
        * client library such as the Amazon S3 Encryption Client
        * client must encrypt data themselves before sending to S3     
        * client must decrypt data themselves when retrieving from S3
        * customer fully manages the keys and encryption cycle
             
### Encryption in transit ###
* Amazon s# exposes: 
    * HTTP endpoint: non encrypted
    * HTTPS endpoint: encryption in flight
* You are free to use endpoint you want, but HTTPS is recommended
* Most clients would use the HTTPS endpoint by default
* HTTPS in mandatory for SSE-C
* Encryption in flight is also called SSL/TLS    

### S3 Security ###
* User based
    * IAM policies - which API calls should be allowed for a specific user from AIM console
* Resource Based
    * Bucket Policies - bucket wide rules from the S3 console - allows cross account
    * Object Access Control List (ACL) - finer grain 
    * Bucket Access Control List (ACL) - less common
* Note!!: an IAM principal can access an S3 object if:
    * the user IAM permission allow it OR the resource policy ALLOWS it
    * AND there's no explicit DENY
    
### S3 Bucket Policy ###
* JSON based policies
    * Resources: buckets and objects
    * Actions: Set of API to Allow or Deny
    * Effect: Allow/Deny 
    * Principal: the account or user to apply the policy to 
* Use S3 bucket for policy to: 
    * Grant public access to the bucket
    * Force objects to be encrypted at upload 
    * Grant access to another account (Cross Account)
* Bucket settings  
    * Bucket public access to buckets and objects granted through
        * new access control lists (ACLs)
        * any access control list (ACLs)
        * new public bucket or access point policies
    * Block public and cross-account to buckets and objects htrough any public bucket or access point policies
    * These setting were created to prevent company data leaks
* S3 Security Other
    * Networking 
        * Support VPC Endpoints 
    * Logging And audit
        * S3 Access Logs can be stored in other S3 bucket
        * API calls can be logged in AWS CloudTrail
    * MFA Delete: MFA (multi factor authentication) can be required in versioned buckets to delete objects
    * Pre-signed URLs: URL that valid only for a limited time(ex. premium video service for logged in users)
        
### S3 Websites ###
https://www.udemy.com/course/aws-certified-developer-associate-dva-c01/learn/lecture/11851546#overview
* S3 can host static websites and have them accessible on the www
* The website URL will be: 
    * <bucket-name>.s3-webiste-<AWS-region>.amazonaws.com
        OR
      <bucket-name>.s3-webiste.<AWS-region>.amazonaws.com
    * If you get a 403 (Forbidden) error, make sure that bucket policy allows public read 
    
### S3 CORS ###
* CORS basics
    ![](images/aim5.jpg)
    * An orgin is a schema (protocol), host (domain) and port
        * e.g. htttp://www.example.com  (implied port 443 for HTTPS, 80 for HTTP)
    * CORS means Cross-Origin Resource Sharing
    * Web browser based mechanism to allow requests to other origins while visiting the main origin
    * Same origin : htttp://www.example.com/app1 and htttp://www.example.com/app2, request from one url to another one is allowed       
    * Different origin (CORS) : htttp://www.example.com and htttp://www.otherexample.com, request from one url to another one is not allowed       
    * The request won't be fulfilled unless the other origin allows for the request, using CORS Headers (ex. Access-Control-Allow-Origin)   
* S3 CORS
    ![](images/aim6.jpg)
    * If a client does a cross-origin request on our S3 bucket, we need to enable the correct CORS headers
    * You can allow for a specific origin or for * (all origins)
 
### S3 Consistency ###
* Read after write consistency
    * As soon as a new object is written, we can retrieve it
    ex: (PUT 200 => GET 200)
    * This is true, except ifi we did a GET before to see if the object exited
    ex: (GET 404 => PUT 200 => GET 404) - eventually consistent
* Eventual Consistency for DELETES and PUTS of existing objects
    * If we read an object after updating, we might get the older version 
    ex:(PUT 200 => PUT 200 => GET 200(might be older version))
    * If we delete an object, we might still be able to retrieve it for a short time
    ex: (DELETE 200 => GET 200)
